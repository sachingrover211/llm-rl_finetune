{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial for optimizing a linear policy for MountainCar Continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "Major Modules:\n",
    "\n",
    "- World: In this case - MountaincarContinuousActionWorld, is the gym environment in which the agent interacts and receives a certain reward for performed actions following a policy. Here, max trajectory length is configurable and is currently set to 300 for an episode.\n",
    "\n",
    "- Agent: This module consists of 3 sub modules namely EpisodeRewardBufferNoBias, LinearPolicy, and LLMBrain. The agent interacts in the world based on actions given by the linear policy. The policy is optimized by the llm brain by injecting agent rollout into the prompt and getting suggested updates to the policy weights. The replay buffer module handles storing weights and episode accumulated rewards to be added to the prompt.\n",
    "\n",
    "Sub Modules:\n",
    " - LLMBrain: Handles interaction with LLM and generates prompt and optimizations based on LLM response for the agent.\n",
    " - LinearPolicy: Stores weights for the model, and generates actions for the agent.\n",
    " - EpisodeRewardBufferNoBias: Used to add (weight, accumulated reward) pair to the buffer when is then fed into the prompt by LLMBrain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "from decimal import Decimal\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from openai import OpenAI\n",
    "from jinja2 import Template\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPISODES=400 # Total number of episodes to train for\n",
    "RENDER_MODE=None # Choose from 'human', 'rgb_array', or None\n",
    "MAX_TRAJ_COUNT=1000 # Maximum number of trajectories to store in buffer for prompt\n",
    "MAX_TRAJ_LENGTH=1000 # Maximum number of steps in a trajectory\n",
    "LLM_MODEL_NAME=\"gpt-4o\" # LLM for optimization, choose from \"o1-preview\",\"gpt-4o\",\"gemini-2.0-flash-exp\",\"gpt-4o-mini\",\"gemini-1.5-flash\",\"gemini-1.5-flash-8b\",\"o3-mini-2025-01-31\"\n",
    "NUM_EVALUATION_EPISODES=20 # Number of episodes to generate agent rollouts for evaluation\n",
    "WARMUP_EPISODES=20 # Number of randomly generated initial episodes\n",
    "SEARCH_STD=1.0 # Step size for LLM to search for optimal parameters during exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_SI_TEMPLATE_STRING = \"\"\"You are an optimization assistant, helping me find the global maximum of a mathematical function.  \n",
    "I will give you the function evaluation and the current iteration number at each step. \n",
    "Your goal is to propose input values that efficiently lead us to the global maximum within a limited number of iterations (300). \n",
    "\n",
    "Here's how we'll interact:\n",
    "\n",
    "1. I will first provide MAX_STEPS (300) along with a few training examples.\n",
    "2. You will provide your response in the following exact format:\n",
    "    * Line 1: a new input 'a: , b: ', aiming to maximize the function's value f(a,b). \n",
    "    * Line 2: a brief (one sentence) explanation of why you chose that input, considering the current iteration.\n",
    "3. I will then provide the function's value f(a,b) at that point, and the current iteration.\n",
    "4. We will repeat steps 2-3 until we reach the maximum number of iterations.\n",
    "\n",
    "Remember:\n",
    "\n",
    "* **Assume no prior knowledge about the function's specific form.**\n",
    "* **Balance Exploitation and Exploration:**  Early on, explore broadly. As iterations increase, focus more on promising regions.\n",
    "* **Be adaptable:**  Your approach might need to change based on the function's behavior and the remaining iterations. \n",
    "* **Do not propose previously seen input pairs.**\n",
    "* **Explore with smaller step size around the historical best values.**\n",
    "* **Each input variable has some interplay with each other**\n",
    "* **Try to use step size of {{ search_std }} when you perform the explorations.**\n",
    "If you think you are stuck in a local maxima or making small increments for too long, \n",
    "try more exploratory values and then eventually exploit new values based on your understanding of the function.\n",
    "\n",
    "\n",
    "The expected optimum value should be over 50. Please do not be trapped in local optimums. Search between -7 and 7.\n",
    "\n",
    "\n",
    "Next, you will see examples of a,b and f(a,b) pairs.\n",
    "{{ episode_reward_buffer_string }}\n",
    "\n",
    "Now you are at iteration {{step_number}}. Please provide the results in the indicated format. Do not provide any additional texts.\"\"\"\n",
    "\n",
    "\n",
    "llm_si_template = Template(LLM_SI_TEMPLATE_STRING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MountainCarContinuous-v0\n",
    "# https://gymnasium.farama.org/environments/classic_control/mountain_car_continuous/\n",
    "\n",
    "class MountaincarContinuousActionWorld():\n",
    "    def __init__(\n",
    "        self,\n",
    "        gym_env_name,\n",
    "        render_mode,\n",
    "        max_traj_length=300,\n",
    "    ):\n",
    "        assert render_mode in [\"human\", \"rgb_array\", None]\n",
    "        self.name = gym_env_name\n",
    "        self.env = gym.make(gym_env_name, render_mode=render_mode)\n",
    "        self.steps = 0\n",
    "        self.accu_reward = 0\n",
    "        self.max_traj_length = max_traj_length\n",
    "\n",
    "    def reset(self):\n",
    "        state, _ = self.env.reset()\n",
    "        self.steps = 0\n",
    "        self.accu_reward = 0\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        self.steps += 1\n",
    "        action = action[0]\n",
    "        state, reward, done, _, _ = self.env.step(action)\n",
    "        self.accu_reward += reward\n",
    "\n",
    "        if self.steps >= self.max_traj_length:\n",
    "            done = True\n",
    "\n",
    "        return state, reward, done\n",
    "\n",
    "    def get_accu_reward(self):\n",
    "        return self.accu_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpisodeRewardBufferNoBias:\n",
    "    \"\"\"\n",
    "    Buffer to store weights and accumulated rewards for each episode. Follows FIFO queue with a maximum size of \"MAX_TRAJ_COUNT\".\n",
    "    \"\"\"\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, weights: np.ndarray, reward):\n",
    "        self.buffer.append((weights, reward))\n",
    "    \n",
    "    def __str__(self):\n",
    "        buffer_table = \"Parameters | Reward\\n\"\n",
    "        for weights, reward in self.buffer:\n",
    "            buffer_table += f\"{weights.reshape(1, -1)} | {reward}\\n\"\n",
    "        return buffer_table\n",
    "\n",
    "    def load(self, folder):\n",
    "        # Find all episode files\n",
    "        all_files = [os.path.join(folder, x) for x in os.listdir(folder) if x.startswith('warmup_rollout')]\n",
    "        all_files.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "\n",
    "        # Load parameters from all episodes\n",
    "        for filename in all_files:\n",
    "            with open(filename, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                parameters = []\n",
    "                for line in lines:\n",
    "                    if \"parameter ends\" in line:\n",
    "                        break\n",
    "                    try:\n",
    "                        parameters.append([float(x) for x in line.split(',')])\n",
    "                    except:\n",
    "                        continue\n",
    "                parameters = np.array(parameters)\n",
    "\n",
    "                rewards = []\n",
    "                for line in lines:\n",
    "                    if \"Total reward\" in line:\n",
    "                        try:\n",
    "                            rewards.append(float(line.split()[-1]))\n",
    "                        except:\n",
    "                            continue\n",
    "                rewards_mean = np.mean(rewards)\n",
    "                self.add(parameters, rewards_mean)\n",
    "                f.close()\n",
    "\n",
    "\n",
    "class LinearPolicy():\n",
    "    \"\"\"\n",
    "    Linear policy for continuous action space. The policy is represented as a (2,1) matrix of weights.\n",
    "    Next action is calculated as the dot product of the state and the weight matrix.\n",
    "    state.T * weight -> action\n",
    "    (1,2) * (2,1) -> (1,1)\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_states, dim_actions):\n",
    "        self.dim_states = dim_states\n",
    "        self.dim_actions = dim_actions\n",
    "        self.weight = np.random.rand(self.dim_states, self.dim_actions)\n",
    "\n",
    "    def initialize_policy(self):\n",
    "        self.weight = np.round((np.random.rand(self.dim_states, self.dim_actions) - 0.5) * 6, 1)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = state.T\n",
    "        return np.matmul(state, self.weight)\n",
    "\n",
    "    def __str__(self):\n",
    "        output = \"Weights:\\n\"\n",
    "        for w in self.weight:\n",
    "            output += \", \".join([str(i) for i in w])\n",
    "            output += \"\\n\"\n",
    "\n",
    "        return output\n",
    "\n",
    "    def update_policy(self, weight_and_bias_list):\n",
    "        if weight_and_bias_list is None:\n",
    "            return\n",
    "        self.weight = np.array(weight_and_bias_list, dtype=np.float64).reshape(-1)\n",
    "        self.weight = np.array([float(Decimal(str(w)).normalize()) \n",
    "                           for w in self.weight], dtype=np.float64)\n",
    "        self.weight = self.weight.reshape(\n",
    "            self.dim_states, self.dim_actions\n",
    "        )\n",
    "\n",
    "\n",
    "class LLMBrain:\n",
    "    \"\"\"\n",
    "    Interacts with LLM for optimization of policy parameters.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm_si_template: Template,\n",
    "        llm_model_name: str,\n",
    "    ):\n",
    "        self.llm_si_template = llm_si_template\n",
    "        self.llm_conversation = []\n",
    "        assert llm_model_name in [\n",
    "            \"o1-preview\",\n",
    "            \"gpt-4o\",\n",
    "            \"gemini-2.0-flash-exp\",\n",
    "            \"gpt-4o-mini\",\n",
    "            \"gemini-1.5-flash\",\n",
    "            \"gemini-1.5-flash-8b\",\n",
    "            \"o3-mini-2025-01-31\",\n",
    "        ]\n",
    "        self.llm_model_name = llm_model_name\n",
    "        if \"gemini\" in llm_model_name:\n",
    "            self.model_group = \"gemini\"\n",
    "            genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "        else:\n",
    "            self.model_group = \"openai\"\n",
    "            self.client = OpenAI()\n",
    "\n",
    "    def reset_llm_conversation(self):\n",
    "        self.llm_conversation = []\n",
    "\n",
    "    def add_llm_conversation(self, text, role):\n",
    "        if self.model_group == \"openai\":\n",
    "            self.llm_conversation.append({\"role\": role, \"content\": text})\n",
    "        else:\n",
    "            self.llm_conversation.append({\"role\": role, \"parts\": text})\n",
    "\n",
    "    def query_llm(self):\n",
    "        for attempt in range(5):\n",
    "            try:\n",
    "                if self.model_group == \"openai\":\n",
    "                    completion = self.client.chat.completions.create(\n",
    "                        model=self.llm_model_name,\n",
    "                        messages=self.llm_conversation,\n",
    "                    )\n",
    "                    response = completion.choices[0].message.content\n",
    "                else:\n",
    "                    model = genai.GenerativeModel(model_name=self.llm_model_name)\n",
    "                    chat_session = model.start_chat(history=self.llm_conversation[:-1])\n",
    "                    response = chat_session.send_message(\n",
    "                        self.llm_conversation[-1][\"parts\"]\n",
    "                    )\n",
    "                    response = response.text\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                print(\"Retrying...\")\n",
    "                if attempt == 4:\n",
    "                    raise Exception(\"Failed\")\n",
    "                else:\n",
    "                    print(\"Waiting for 60 seconds before retrying...\")\n",
    "                    time.sleep(60)\n",
    "\n",
    "            if self.model_group == \"openai\":\n",
    "                # add the response to self.llm_conversation\n",
    "                self.add_llm_conversation(response, \"assistant\")\n",
    "            else:\n",
    "                self.add_llm_conversation(response, \"model\")\n",
    "\n",
    "            return response\n",
    "\n",
    "    def parse_parameters(self, parameters_string):\n",
    "        new_parameters_list = []\n",
    "\n",
    "        # Update the Q-table based on the new Q-table\n",
    "        for row in parameters_string.split(\"\\n\"):\n",
    "            if row.strip().strip(\",\"):\n",
    "                try:\n",
    "                    parameters_row = [\n",
    "                        float(x.strip().strip(\",\")) for x in row.split(\",\")\n",
    "                    ]\n",
    "                    new_parameters_list.append(parameters_row)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "        return new_parameters_list\n",
    "\n",
    "    def llm_update_parameters_num_optim(\n",
    "        self, episode_reward_buffer, parse_parameters, step_number, search_std\n",
    "    ):\n",
    "        self.reset_llm_conversation()\n",
    "\n",
    "        system_prompt = self.llm_si_template.render(\n",
    "            {\n",
    "                \"episode_reward_buffer_string\": str(episode_reward_buffer),\n",
    "                \"step_number\": str(step_number),\n",
    "                \"search_std\": str(search_std),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        self.add_llm_conversation(system_prompt, \"user\")\n",
    "        new_parameters_with_reasoning = self.query_llm()\n",
    "        print(system_prompt)\n",
    "\n",
    "        new_parameters_list = parse_parameters(new_parameters_with_reasoning)\n",
    "\n",
    "        return (\n",
    "            new_parameters_list,\n",
    "            \"system:\\n\"\n",
    "            + system_prompt\n",
    "            + \"\\n\\n\\nLLM:\\n\"\n",
    "            + new_parameters_with_reasoning,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MountaincarContinuousActionLLMNumOptimAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        logdir,\n",
    "        dim_action,\n",
    "        dim_state,\n",
    "        max_traj_count,\n",
    "        llm_si_template,\n",
    "        llm_model_name,\n",
    "        num_evaluation_episodes,\n",
    "    ):\n",
    "        self.policy = LinearPolicy(dim_actions=dim_action, dim_states=dim_state)\n",
    "        self.replay_buffer = EpisodeRewardBufferNoBias(max_size=max_traj_count)\n",
    "        self.llm_brain = LLMBrain(\n",
    "            llm_si_template, llm_model_name\n",
    "        )\n",
    "        self.logdir = logdir\n",
    "        self.num_evaluation_episodes = num_evaluation_episodes\n",
    "        self.training_episodes = 0\n",
    "\n",
    "    def rollout_episode(self, world: MountaincarContinuousActionWorld, logging_file, record=True):\n",
    "        state = world.reset()\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        logging_file.write(f\"state | action | reward\\n\")\n",
    "        done = False\n",
    "        step_idx = 0\n",
    "        while not done:\n",
    "            action = self.policy.get_action(state.T)\n",
    "            action = np.reshape(action, (1, 1))\n",
    "            next_state, reward, done = world.step(action)\n",
    "            logging_file.write(f\"{state.T[0]} | {action[0]} | {reward}\\n\")\n",
    "            state = next_state\n",
    "            step_idx += 1\n",
    "        logging_file.write(f\"Total reward: {world.get_accu_reward()}\\n\")\n",
    "        if record:\n",
    "            self.replay_buffer.add(\n",
    "                self.policy.weight, world.get_accu_reward()\n",
    "            )\n",
    "        return world.get_accu_reward()\n",
    "\n",
    "    def random_warmup(self, world: MountaincarContinuousActionWorld, logdir, num_episodes):\n",
    "        for episode in range(num_episodes):\n",
    "            self.policy.initialize_policy()\n",
    "            # Run the episode and collect the trajectory\n",
    "            print(f\"Rolling out warmup episode {episode}...\")\n",
    "            logging_filename = f\"{logdir}/warmup_rollout_{episode}.txt\"\n",
    "            logging_file = open(logging_filename, \"w\")\n",
    "            result = self.rollout_episode(world, logging_file)\n",
    "            print(f\"Result: {result}\")\n",
    "\n",
    "    def train_policy(self, world: MountaincarContinuousActionWorld, logdir, search_std):\n",
    "\n",
    "        def parse_parameters(input_text):\n",
    "            # This regex looks for integers or floating-point numbers (including optional sign)\n",
    "            s = input_text.split(\"\\n\")[0]\n",
    "            pattern = r\"[-+]?\\d+(?:\\.\\d+)?\"\n",
    "            matches = re.findall(pattern, s)\n",
    "\n",
    "            # Convert matched strings to float (or int if you prefer to differentiate)\n",
    "            results = []\n",
    "            for match in matches:\n",
    "                results.append(float(match))\n",
    "            assert len(results) == 2\n",
    "            return np.array(results).reshape((2, 1))\n",
    "\n",
    "        def str_2d_examples(replay_buffer: EpisodeRewardBufferNoBias):\n",
    "\n",
    "            all_parameters = []\n",
    "            for weights, reward in replay_buffer.buffer:\n",
    "                parameters = weights\n",
    "                all_parameters.append((parameters.reshape(-1), reward))\n",
    "\n",
    "            text = \"\"\n",
    "            for parameters, reward in all_parameters:\n",
    "                l = \"\"\n",
    "                for i in range(2):\n",
    "                    l += f'{\"abcdefghijklmnopqr\"[i]}: {parameters[i]}; '\n",
    "                fxy = reward\n",
    "                l += f\"f(a,b): {fxy}\\n\"\n",
    "                text += l\n",
    "            return text\n",
    "\n",
    "        # Run the episode and collect the trajectory\n",
    "        print(f\"Rolling out episode {self.training_episodes}...\")\n",
    "        logging_filename = f\"{logdir}/training_rollout.txt\"\n",
    "        logging_file = open(logging_filename, \"w\")\n",
    "        result = self.rollout_episode(world, logging_file)\n",
    "        print(f\"Result: {result}\")\n",
    "\n",
    "        # Update the policy using llm_brain, q_table and replay_buffer\n",
    "        print(\"Updating the policy...\")\n",
    "        new_parameter_list, reasoning = self.llm_brain.llm_update_parameters_num_optim(\n",
    "            str_2d_examples(self.replay_buffer),\n",
    "            parse_parameters,\n",
    "            self.training_episodes,\n",
    "            search_std,\n",
    "        )\n",
    "\n",
    "        self.policy.update_policy(new_parameter_list)\n",
    "        logging_q_filename = f\"{logdir}/parameters.txt\"\n",
    "        logging_q_file = open(logging_q_filename, \"w\")\n",
    "        logging_q_file.write(str(self.policy))\n",
    "        logging_q_file.close()\n",
    "        q_reasoning_filename = f\"{logdir}/parameters_reasoning.txt\"\n",
    "        q_reasoning_file = open(q_reasoning_filename, \"w\")\n",
    "        q_reasoning_file.write(reasoning)\n",
    "        q_reasoning_file.close()\n",
    "        print(\"Policy updated!\")\n",
    "\n",
    "        self.training_episodes += 1\n",
    "\n",
    "    def evaluate_policy(self, world: MountaincarContinuousActionWorld, logdir):\n",
    "        results = []\n",
    "        for idx in range(self.num_evaluation_episodes):\n",
    "            logging_filename = f\"{logdir}/evaluation_rollout_{idx}.txt\"\n",
    "            logging_file = open(logging_filename, \"w\")\n",
    "            result = self.rollout_episode(world, logging_file, record=False)\n",
    "            results.append(result)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Generates a log dir with following structure inside `logs/mountaincar_continuous_action_llm_num_optim_400_std_1_expected_r_no_bias_range/`:\n",
    " - episode_0\n",
    "    * evaluation_rollout_0.txt\n",
    "    * .\n",
    "    * evaluation_rollout_n.txt - contains state, action, reward trajectory of the agent \n",
    "    * parameters_reasoning.txt - LLM reasoning text for episode 0\n",
    "    * parameters.txt - consists of weights for the policy after episode 0\n",
    "    * training_rollout.txt\n",
    " - episode_1\n",
    " - .\n",
    " - episode_n\n",
    " - warmup\n",
    "\n",
    "##### Training loop:\n",
    " - Generate warmup episodes, add to replay buffer\n",
    " - Update the prompt with replay buffer\n",
    "      - Request LLM for new parameters\n",
    "      - Update agent's policy with new parameters\n",
    "      - evaluate new policy and add to replay buffer\n",
    "      - repeat\n",
    " - end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_loop(\n",
    "    num_episodes=NUM_EPISODES,\n",
    "    gym_env_name=\"MountainCarContinuous-v0\",\n",
    "    render_mode=RENDER_MODE,\n",
    "    logdir=\"logs/mountaincar_continuous_action_llm_num_optim_400_std_1_expected_r_no_bias_range\",\n",
    "    dim_actions=1,\n",
    "    dim_states=2,\n",
    "    max_traj_count=MAX_TRAJ_COUNT,\n",
    "    max_traj_length=MAX_TRAJ_LENGTH,\n",
    "    llm_model_name=LLM_MODEL_NAME,\n",
    "    num_evaluation_episodes=NUM_EVALUATION_EPISODES,\n",
    "    warmup_episodes=WARMUP_EPISODES,\n",
    "    warmup_dir=None,\n",
    "    search_std=SEARCH_STD,\n",
    "):\n",
    "    world = MountaincarContinuousActionWorld(\n",
    "        gym_env_name, \n",
    "        render_mode, \n",
    "        max_traj_length,\n",
    "    )\n",
    "    agent = MountaincarContinuousActionLLMNumOptimAgent(\n",
    "        logdir,\n",
    "        dim_actions,\n",
    "        dim_states,\n",
    "        max_traj_count,\n",
    "        llm_si_template,\n",
    "        llm_model_name,\n",
    "        num_evaluation_episodes,\n",
    "    )\n",
    "\n",
    "    if not warmup_dir:\n",
    "        warmup_dir = f\"{logdir}/warmup\"\n",
    "        os.makedirs(warmup_dir, exist_ok=True)\n",
    "        agent.random_warmup(world, warmup_dir, warmup_episodes)\n",
    "    else:\n",
    "        agent.replay_buffer.load(warmup_dir)\n",
    "    for episode in range(num_episodes):\n",
    "        print(f\"Episode: {episode}\")\n",
    "        # create log dir\n",
    "        curr_episode_dir = f\"{logdir}/episode_{episode}\"\n",
    "        print(f\"Creating log directory: {curr_episode_dir}\")\n",
    "        os.makedirs(curr_episode_dir, exist_ok=True)\n",
    "        for trial_idx in range(5):\n",
    "            try:\n",
    "                agent.train_policy(world, curr_episode_dir, search_std)\n",
    "                print(f\"{trial_idx + 1}th trial attempt succeeded in training\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"{trial_idx + 1}th trial attempt failed with error in training: {e}\")\n",
    "                continue\n",
    "        results = agent.evaluate_policy(world, curr_episode_dir)\n",
    "        print(f\"Episode {episode} Evaluation Results: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m             filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(logdir, episode, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_rollout.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     38\u001b[0m             plot_episode(filename, episode)\n\u001b[0;32m---> 40\u001b[0m \u001b[43mvisualize_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogs/mountaincar_continuous_action_llm_num_optim_400_std_1_expected_r_no_bias_range\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 38\u001b[0m, in \u001b[0;36mvisualize_policy\u001b[0;34m(logdir)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m episode\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepisode\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     37\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(logdir, episode, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_rollout.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m     \u001b[43mplot_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 21\u001b[0m, in \u001b[0;36mvisualize_policy.<locals>.plot_episode\u001b[0;34m(filename, episode)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mplot_episode\u001b[39m(filename, episode):\n\u001b[0;32m---> 21\u001b[0m     state, action, reward \u001b[38;5;241m=\u001b[39m \u001b[43mparse_log_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m     23\u001b[0m     plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 16\u001b[0m, in \u001b[0;36mvisualize_policy.<locals>.parse_log_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m line:\n\u001b[1;32m     15\u001b[0m         s, a, _ \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m         state\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(\u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m))\n\u001b[1;32m     17\u001b[0m         action\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(a\u001b[38;5;241m.\u001b[39msplit()[\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m state, action, reward\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "def visualize_policy(logdir):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import os\n",
    "\n",
    "    def parse_log_file(filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            state = []\n",
    "            action = []\n",
    "            reward = []\n",
    "            for line in lines:\n",
    "                if \"Total reward\" in line:\n",
    "                    reward.append(float(line.split()[-1]))\n",
    "                elif \"|\" in line:\n",
    "                    s, a, _ = line.split(\"|\")\n",
    "                    state.append(float(s.split()[1]))\n",
    "                    action.append(float(a.split()[1]))\n",
    "            return state, action, reward\n",
    "\n",
    "    def plot_episode(filename, episode):\n",
    "        state, action, reward = parse_log_file(filename)\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(state, label='State')\n",
    "        plt.plot(action, label='Action')\n",
    "        plt.title(f'Episode {episode} State and Action')\n",
    "        plt.legend()\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(reward, label='Reward')\n",
    "        plt.title(f'Episode {episode} Reward')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    for episode in os.listdir(logdir):\n",
    "        if episode.startswith('episode'):\n",
    "            filename = os.path.join(logdir, episode, 'training_rollout.txt')\n",
    "            plot_episode(filename, episode)\n",
    "        \n",
    "visualize_policy(\"logs/mountaincar_continuous_action_llm_num_optim_400_std_1_expected_r_no_bias_range\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

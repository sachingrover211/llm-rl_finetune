{% raw %}
Below is an improved prompt that more explicitly guides the model to perform step-by-step, numeric-focused analysis and propose new parameters. Feel free to tailor any part as needed:

**Prompt:**

You are a **numerical global optimizer**, tasked with finding the global maximum of a mathematical function \( f(\text{params}) \). The variable **params** is a 33-dimensional float array \([p_0, p_1, \ldots, p_{32}]\), each value constrained to the range \([-3.0, 3.0]\), with a precision of **one decimal place**.

We will iterate in a loop. In each step:
1. You will receive:
   - The **iteration number**.
   - The **function value** \( f(\text{params}) \) at the last **params** you proposed.
   - A **history** of all previously tried \((\text{params}, f(\text{params}))\) pairs.

2. In your answer, you will produce exactly **three sections**:

   ---
   **Section 1: Analysis of Last Evaluation**

   - Summarize how your previously proposed **params** performed.  
   - Discuss any insight gained from the resulting \( f(\text{params}) \).  
   - State explicitly: “The last hypothesis was <...>. The result was <...>. This suggests <...>.”  
   - Keep this concise, but ensure you address why the result was good/bad and what it means for your approach.

   ---
   **Section 2: Mathematical / Algorithmic Reasoning**

   - You **must** provide numeric or formula-based detail in your explanation:
     1. **Show at least one mini-calculation** (e.g., approximate gradient, how you formed candidates in a genetic algorithm, etc.).  
     2. **Use explicit step sizes** (e.g., ±0.3) and specify why these values are chosen.  
     3. If you use phrases like “enlarge step size,” you **must** specify the exact step (e.g., from 0.2 to 0.3).  
     4. If you mention “local exploitation,” explain exactly how many candidate points you generate in a “neighborhood” and the numeric radius of that neighborhood.  
     5. **No vague language** like “slightly enlarge.” Replace it with numeric values (e.g., “enlarge each dimension’s step from ±0.1 to ±0.3”).  
     6. Provide an **illustrative numeric example** using your proposed approach (e.g., partial derivatives or candidate param sets).


   ---
   **Section 3 (exactly 1 line): Propose Next params**

   - Provide the **new** 33-dimensional **params** as **`params[0]: <value>, params[1]: <value>, ..., params[32]: <value>`**.  
   - Each value must be:
     - **Within \([-3.0, 3.0]\)**.  
     - Rounded to **1 decimal place**.  
     - Chosen with a minimum step of **0.1** from previously tried values (avoid exact repetition of old solutions).  
   - *Remember*, your goal is to push towards the global maximum, and the global optimum is around ~2000.0. If you detect a local optimum, consider a larger jump.

3. I will evaluate your proposed **params** on the hidden function \( f(\text{params}) \) and provide the next iteration's \( f(\text{params}) \) value, along with the updated history. We repeat until we reach 1000 iterations or we converge.

**Key Points to Remember:**
- **One decimal place** only.
- **Do not** propose any previously used **params** sets.
- A single step cannot be smaller than **0.1** in *any* dimension if you want to move from a previously tried point.  
- Aim for the global maximum, presumably ~2000.0. Avoid getting trapped in local maxima.

Below is an **example** of the style expected:

> *“We have two past evaluations at \(\mathbf{p}^{(1)} = [1.0, -1.0,\ldots]\) and \(\mathbf{p}^{(2)} = [1.1, -0.9,\ldots]\) with \(f(\mathbf{p}^{(1)}) = 100\) and \(f(\mathbf{p}^{(2)}) = 110\). From these, we approximate \(\frac{\partial f}{\partial p_0} \approx \frac{110 - 100}{1.1 - 1.0} = 100\). Thus, we increase \(p_0\) by 0.3 (since our new step size is 0.3). For the other parameters, we do a random step in the range \([-0.2, 0.2]\). We do this because we suspect a partial local optimum in dimension 0, but want to explore further in the other dimensions.”*

Use this as a template for the **level of numerical specificity** required.


When generating your solution, you must follow these rules for **Section 2: Mathematical / Algorithmic Reasoning**:

1. **Show a mini-calculation for each parameter change.**  
   - For example, if you decide to adjust `params[2]` from 0.0 to 0.1, you must explain precisely how you arrived at +0.1.  
     - **Use partial derivatives, differences in function values, or any numeric method.** Provide at least one arithmetic equation (e.g., finite differences, random sampling offsets, or a crossover formula if using a genetic approach).  

2. **If you fix certain parameters (`params[3]`, `params[5]`, etc.)**, explain numerically why they’re “fixed” or “critical.”  
   - E.g., “Because partial derivative wrt `params[3]` is near zero, we keep it at 2.0.”  

3. **Avoid phrases like ‘capitalize on positive trajectory’ or ‘introduce small variations.’**  
   - Instead, **provide numeric steps** (e.g., “we add ±0.1 based on standard deviation or partial derivative magnitude of 0.4”).  

4. **Include at least one of the numeric analysis** in your explanation:  
   - For instance, if using a gradient approximation, show something like:  
     `∂f/∂p0 ≈ (f(p0 + δ) - f(p0)) / δ = (120 - 100) / 0.2 = 100`  
     so we choose to move `params[0]` by +0.3 in the positive gradient direction.  

5. **Be explicit about step sizes**:  
   - If you say “±0.1 variation,” detail how or why you chose ±0.1: “We pick ±0.1 because it is half of the previously used step ±0.2, aiming for finer control.”  

6. **No purely qualitative justification**:  
   - You must link each param change to some numeric basis—random sampling range, gradient, prior function values, or standard deviation from a population of solutions, etc.

---

**Example of the Rigor Expected:**

> “From the last two evaluations at `params[2] = 0.0` yielding f=100, and `params[2] = 0.1` yielding f=108, we approximate the partial derivative wrt `params[2]` as:  
> \[
>  \frac{108 - 100}{0.1 - 0.0} = 80.
> \]
> Since the gradient is strongly positive, we propose moving `params[2]` further from 0.1 to 0.3 (a +0.2 step). We choose 0.2 because it is 25% of the max range (±3.0) and ensures a balanced exploration. For `params[3]`, the derivative is near zero, so we keep it constant at -1.5.”  

In other words, please provide **arithmetic or formula-based justifications** for each param change, not just high-level qualitative language.


{% endraw %}

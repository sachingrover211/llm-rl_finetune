You are an optimization assistant, helping me find the global optimal policy of Pong environment. This is a discrete action space problem where there the agent and the environment are hitting the ball to each other. The agent is a vertical pad on the left side (x=0), while the environment is a vertical pad on the right side (x=800). Both pads can move vertically betwee y=0 and y=600. The goal is to hit the ball to the opponent's side with the pad (pad length is 80). Avoid missing the ball.
The observation is 5 dimensional, consisting of y-position of the agent pad, ball x-position, ball y-position, ball x-velocity, and ball y-velocity. 
The action is the movement of the agent pad. There are 3 actions: move up (+y), stay, move down (-y).
The reward increases by 1 for each time the agent hits the ball to the opponent's side. The episode ends when the agent misses the ball, or the agent hits the ball to the opponent's side 3 times.
The agent is using a linear policy, where the action is defined as a linear combination of the observations: 
* move_up_energy = params[0] * y_position + params[1] * ball_x_position + params[2] * ball_y_position + params[3] * ball_x_velocity + params[4] * ball_y_velocity.
* stay_energy = params[5] * y_position + params[6] * ball_x_position + params[7] * ball_y_position + params[8] * ball_x_velocity + params[9] * ball_y_velocity.
* move_down_energy = params[10] * y_position + params[11] * ball_x_position + params[12] * ball_y_position + params[13] * ball_x_velocity + params[14] * ball_y_velocity.
The agent will choose the action with the highest energy value.
Your goal is to propose the policy parameters [0-14] that efficiently lead us to the global maximal reward within a limited number of iterations (300). 

# Regarding the parameters **params**:
**params** is an array of 15 integer numbers.
**params** are integers.
**params** values are in the range of [-3.0, 3.0].

# Here's how we'll interact:
1. I will first provide MAX_STEPS (200) along with a few training examples.
2. You will provide your response in the following exact format:
    * Line 1: a new input 'params[0]: , params[1]: , params[2]: ,..., params[14]: ', aiming to maximize the reward f(params). 
    Please propose params values in the range of [-3.0, 3.0].
    * Line 2: detailed explanation of why you chose that input.
3. I will then provide the reward f(params) of your proposed parameters [0-14], and the current iteration.
4. We will repeat steps 2-3 until we reach the maximum number of iterations.

# Remember:
1. **Do not propose previously seen params.**
2. **Use search step size of 1!!!**
3. **The global optimum should be around 3.0.** If you are below that, this is just a local optimum. You should explore instead of exploiting.
4. Search both positive and negative values. 


Next, you will see examples of params and f(params) pairs.
{{ episode_reward_buffer_string }}

Now you are at iteration {{step_number}} out of 200. Please provide the results in the indicated format. Do not provide any additional texts.
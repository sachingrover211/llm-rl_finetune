You are good global RL policy optimizer, helping me find the global optimal policy in the following environment:

# Environment:
{% include env_description %}

# Regarding the parameters **params**:
**params** is an array of {{ rank }} float numbers.
**params** values are in the range of [-10.0, 10.0] with 1 decimal place.
params represent a linear policy. 
f(params) is the episodic reward of the policy.

# Here's how we'll interact:
1. I will first provide MAX_STEPS ({{ episodes }}) along with a few training examples.
2. You will provide your response in the following exact format:
    * Line 1: a new input 'params[0]: ; params[1]: ; params[2]: ;...; params[{{ rank - 1 }}]: ', aiming to maximize the function's value f(params). 
    Please propose params values in the range of [-10.0, 10.0], with 1 decimal place.
    * Line 2: detailed explanation of why you chose that input.
3. I will then provide the function's value f(params) at that point, and the current iteration.
4. We will repeat steps 2-3 until we reach the maximum number of iterations.

# Remember:
1. **Do not propose previously seen params.**
2. **The global optimum should be around {{ optimum }}.** If you are below that, this is just a local optimum. You should explore instead of exploiting.
3. Search both positive and negative values. **During exploration, use search step size of {{ step_size }}**.


Next, you will see examples of params, there episodic reward f(params), and the trajectories the params yield.
{{ episode_reward_buffer_string }}

Now you are at iteration {{step_number}} out of {{ episodes }}. Please provide the results in the indicated format. Do not provide any additional texts.

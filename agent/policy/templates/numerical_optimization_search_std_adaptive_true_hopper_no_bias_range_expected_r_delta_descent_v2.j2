You are good global gradient descent optimizer, helping me find the global minimum of a cost function f(params).
I will give you the function evaluation and the current iteration number at each step. 
Your goal is to propose the delta (**value change**) of the params that such that f(params + delta) is smaller than all previous f(params). We will have a maximum of 300 iterations. 

# Regarding the parameters **params**:
**params** is an array of 33 float numbers.
**params** values are in the range of [-3.0, 3.0].

# Here's how we'll interact:
1. I will first provide MAX_STEPS (300) along with a few training examples.
2. You will provide your response in the following exact format:
    * Line 1: a new set of params 'delta[0]: , delta[1]: , delta[2]: ,..., delta[32]: '. The parameters will be updated as params[i] = params[i] + delta[i]. The goal is to minimize the function's value f(params + delta).
    Please make sure the updated params values are in the range of [-3.0, 3.0].
    * Line 2: detailed explanation of why you chose that delta.
3. I will then provide the cost function's value f(new_params) at that point, as the last row in the examples.
4. We will repeat steps 2-3 until we reach the maximum number of iterations.

# Remember:
1. **Do not propose delta such that the updated params are previous seen.**
2. **The global optimum should be around 400-500.** If you are largely above that, this is just a local optimum. You should explore instead of exploiting.
3. Search both positive and negative values. 


Next, you will see examples of params and f(params) pairs.
{{ episode_reward_buffer_string }}

Now you are at iteration {{step_number}} out of 300. Please provide your **proposed delta** in the indicated format. Do not provide any additional texts. Do not directly provide the updated params.